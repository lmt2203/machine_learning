---
title: "Regression Trees and Classification Trees"
author: "Yifei Sun"
output: 
   html_document:
     toc: true
--- 

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
# for data
library(ISLR)
library(mlbench)

# for comparison
library(caret)

library(rpart) #implement CART algorithm (classification, regression tree)
library(rpart.plot) # for tree diagram
library(party) #conditional inference tree CIT, no pruning needed
library(partykit) # useful for visualization of tree diagram

library(plotmo)
library(pROC)

#to fit random forest models
library(randomForest)

library(ranger) # fast implementation of the random forest, for larger sample size
library(pdp)

library(gbm)

```



# Regression Trees

Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)  #these methods cannot handle missing data so omit na for now  

set.seed(2021)
trRows <- createDataPartition(Hitters$Salary,
                              p = .75,
                              list = F)
```

## The CART approach

We first apply the regression tree method to the Hitters data. `cp` is the complexity parameter. The default value for `cp` is 0.01. Sometimes the default value may over prune the tree. `rpart` package does 10-fold CV for you. cp = 0 => most complex tree. 

Objective function: R(T) + $\alpha$|T| where R(T) = loss function. `rpart` use slightly different function of objective function: R(T) + cp*|T|*R(T_root). 

```{r}
set.seed(1)
tree1 <- rpart(formula = Salary ~ . , 
               data = Hitters, 
               subset = trRows, #to indicate we're using training data
               control = rpart.control(cp = 0)) # to make sure we dont miss anything set cp as small as possible

# If any split does not increase R-squared by cp => we don't consider that split
# cp = 1 => no split (bc 1 is the largest value of R-squared)
# cp = 0 => large tree (no pruning)
# arguments: minsplit (default = 20), minsbucket (minsplit/3): when to stop. n-min obs in each terminal node. Only change if have a large dataset where these values seem too small. maxdepth = 30 by default (reasonably large so no need to change)



# plot(tree1)
# text(tree1)

rpart.plot(tree1) #get a large tree WITHOUT PRUNING (cp = 0, according to specified cp). This is NOT the final tree. Yield mean of y on each nodes. 
```

We get a smaller tree by increasing the complexity parameter.

```{r, fig.height=3, fig.width=4}
# just for illustration, don't use this
set.seed(1)
tree2 <- rpart(Salary ~ . ,  
               data = Hitters, subset = trRows,
               control = rpart.control(cp = 0.1))
rpart.plot(tree2)
```

### Pruning

We next apply cost complexity pruning to obtain a tree with the right size. The functions `printcp()` and `plotcp()` give the set of possible cost-complexity prunings of a tree from a nested set. For the geometric means of the intervals of values of `cp` for which a pruning is optimal, a cross-validation has been done in the initial construction by `rpart()`. 

The `cptable` in the fit contains the mean and standard deviation of the errors in the cross-validated prediction against each of the geometric means, and these are plotted by `plotcp()`. `Rel error` (relative error) is `\(1 – R^2\)`. The x-error is the cross-validation error generated by built-in cross validation. A good choice of `cp` for pruning is often the leftmost value for which the mean lies below the horizontal line.

```{r}
printcp(tree1)  #variables used in tree construction are usually important
# number of terminal nodes = nsplit + 1
# pick one with the smallest error 
cpTable <- tree1$cptable
plotcp(tree1)
```

Prune the tree based on the `cp` table.

```{r}
# minimum cross-validation error
minErr <- which.min(cpTable[,4])
tree3 <- prune(tree1, cp = cpTable[minErr,1])  #final tree

rpart.plot(tree3)

plot(as.party(tree3)) #boxplot show the distribution of the data

summary(tree3)
# 2 types of splits: primary splits and surrogate splits 
# surrogate splits is used when data contain missing values 

with(Hitters[trRows,], table(cut(CRBI, c(-Inf, 307.5, Inf)), #primary
                             cut(CAtBat, c(-Inf, 2316.5, Inf)))) #surrogate
# 2x2 table 
```


```{r}
# 1SE rule
tree4 <- prune(tree1, 
               cp = cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])

rpart.plot(tree4)
```

Finally, the function `predict()` can be used for prediction from a fitted `rpart` object.

```{r}
predict(tree3, newdata = Hitters[-trRows,])
```

### Missing data

```{r}
Hitters2 <- Hitters
Hitters2$CRBI[sample(1:nrow(Hitters2), 50)] <- NA

set.seed(1)
tree_m <- rpart(Salary ~ . ,  
                data = Hitters2,
                subset = trRows,
                control = rpart.control(cp = 0))

cpTable_m <- tree_m$cptable
tree2_m <- prune(tree_m, cp = cpTable_m[which.min(cpTable_m[,4]),1])

summary(tree_m, cp = cpTable_m[which.min(cpTable_m[,4]),1])
head(predict(tree2_m, newdata = Hitters2[-trRows,]))
```


## Conditional inference trees (CIT)

The implementation utilizes a unified framework for conditional inference, or permutation tests. Unlike CART, the stopping criterion is based on p-values. A split is implemented when (1 - p-value) exceeds the value given by `mincriterion` as specified in `ctree_control()`. This approach ensures that the right-sized tree is grown without additional pruning or cross-validation, but can stop early. At each step, the splitting variable is selected as the input variable with strongest association to the response (measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response). Such a splitting procedure can avoid a variable selection bias towards predictors with many possible cutpoints.

```{r, fig.height=4, fig.width=8}
tree5 <- ctree(Salary ~ . , Hitters,
               subset = trRows)
plot(tree5)
```

Note that `tree5` is a `party` object. The function `predict()` can be used for prediction from a fitted `party` object.

```{r}
head(predict(tree5, newdata = Hitters[-trRows,]))
```

## `caret`


```{r}
ctrl <- trainControl(method = "cv")

set.seed(1)
rpart.fit <- train(Salary ~ . , 
                   Hitters[trRows,], 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 50))),
                   trControl = ctrl)
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

We can also fit a conditional inference tree model. The tuning parameter is `mincriterion`.

```{r, fig.height=4, fig.width=8}
set.seed(1)
ctree.fit <- train(Salary ~ . , 
                   Hitters[trRows,], 
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-6, -2, length = 50))),
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)
```


```{r}
summary(resamples(list(rpart.fit, ctree.fit)))
```

```{r}
RMSE(predict(rpart.fit, newdata = Hitters[-trRows,]), Hitters$Salary[-trRows])
RMSE(predict(ctree.fit, newdata = Hitters[-trRows,]), Hitters$Salary[-trRows])
```

# Classification trees

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. 

```{r}
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes

dat$diabetes <- factor(dat$diabetes, c("pos", "neg"))

set.seed(1)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 2/3,
                                list = FALSE)
```


## `rpart`

```{r}
set.seed(1)
tree1 <- rpart(formula = diabetes ~ . , 
               data = dat,
               subset = rowTrain, 
               control = rpart.control(cp = 0))

cpTable <- printcp(tree1)
plotcp(tree1)

# minimum cross-validation error; may also use the 1SE rule
minErr <- which.min(cpTable[,4])
tree2 <- prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree2)
summary(tree2)
```

## `ctree`

```{r, fig.height=6, fig.width=10}
tree2 <- ctree(formula = diabetes ~ . , 
               data = dat,
               subset = rowTrain)

plot(tree2)
```
 
## `caret`

### CART

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart.fit <- train(diabetes ~ . , 
                   dat, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

### CIT

```{r}
set.seed(1)
ctree.fit <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -1, length = 50))),
                   metric = "ROC",
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
```

```{r, fig.width=15, fig.height=6}
plot(ctree.fit$finalModel)
summary(resamples(list(rpart.fit, ctree.fit)))
```



```{r}
rpart.pred <- predict(tree1, newdata = dat[-rowTrain,])[,1]

rpart.pred2 <- predict(rpart.fit, newdata = dat[-rowTrain,],
                       type = "prob")[,1]

ctree.pred <- predict(ctree.fit, newdata = dat[-rowTrain,],
                       type = "prob")[,1]

roc.rpart <- roc(dat$diabetes[-rowTrain], rpart.pred2)
roc.ctree <- roc(dat$diabetes[-rowTrain], ctree.pred)

auc <- c(roc.rpart$auc[1], roc.ctree$auc[1])

plot(roc.rpart, legacy.axes = TRUE)
plot(roc.ctree, col = 2, add = TRUE)


modelNames <- c("rpart","ctree")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:2, lwd = 2)
```


# Bagging and Random Forests

The function `randomForest()` implements Breiman's random forest algorithm (based on Breiman and cutler's original Fortran code) for classification and regression. `ranger()` is a fast implementation of Breiman's random forests, particularly suited for high dimensional data.

```{r}
set.seed(1)

bagging <- randomForest(Salary ~ .,
                        Hitters,
                        subset = trRows,
                        mtry = 19) #number of predictors in this eg
# nodesize = min number of nodes, reg 5 class 1 default

set.seed(1)
rf <- randomForest(Salary ~ .,
                   Hitters,
                   subset = trRows,
                   mtry = 6) # in the slides, supposed to perform well

# fast implementation
set.seed(1)
rf2 <- ranger(Salary ~ . ,
              Hitters[trRows,],
              mtry = 6)
#min.node.size 

pred.rf <- predict(rf, newdata = Hitters[-trRows,])
pred.rf2 <- predict(rf2, data = Hitters[-trRows,])$predictions

RMSE(pred.rf, Hitters$Salary[-trRows])

RMSE(pred.rf2, Hitters$Salary[-trRows])                        
              
```


# Boosting

We first fit a gradient boosting model with Gaussian loss function

```{r}
set.seed(1)

bst <- gbm(Salary ~ .,
           Hitters[trRows,],
           distribution = "gaussian",
           n.trees = 5000, #beta
           interaction.depth = 3, #d
           shrinkage = 0.005, #lambda
           cv.folds = 10,
           n.cores = 2)
#bag.fraction = 0.5 (50% of your data is used to grow tree)
```

We plot loss function as a result of number of trees added to the ensemble

```{r}
gbm.perf(bst, method = "cv")
```

## Grid search using `caret`

We use the fast implementation of random forest when tuning the model.

```{r}
ctrl <- trainControl(method = "cv") 

# Try more if possible
rf.grid <- expand.grid(mtry = 1:19,
                       splitrule = "variance",
                       min.node.size = 1:6)
set.seed(1)
rf.fit <- train(Salary ~ . , 
                Hitters[trRows,], 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)
```

We then tune the `gbm` model.

```{r}
# Try more 
gbm.grid <- expand.grid(n.trees = c(2000,3000,4000), #beta
                        interaction.depth = 1:4, #d
                        shrinkage = c(0.001,0.003,0.005), #lambda
                        n.minobsinnode = c(1,10))
set.seed(1)
gbm.fit <- train(Salary ~ . , 
                 Hitters[trRows,], 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)
```

It takes a while to train the `gbm` even with a rough tuning grid. The `xgboost` package provides an efficient implementation of gradient boosting framework (approximately 10x faster than `gbm`). You can find much useful information here: https://github.com/dmlc/xgboost/tree/master/demo.

Compare the cross-validation performance. You can also compare with other models that we fitted before.

```{r}
resamp <- resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resamp)
```

## Global interpretation

### Variable importance

We can extract the variable importance from the fitted models. In what follows, the first measure is computed from permuting OOB data. The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For regression, node impurity is measured by residual sum of squares.

```{r}
set.seed(1)
rf2.final.per <- ranger(Salary ~ . , 
                        Hitters[trRows,],
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))

set.seed(1)
rf2.final.imp <- ranger(Salary ~ . , 
                        Hitters[trRows,],
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))
```

Variable importance from boosting can be obtained using the `summary()` function.

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### Partial dependence plots 

After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. For this we can use partial dependence plots (PDPs).

PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. The PDP plot below displays the average change in predicted `Salary` as we vary `CRBI` while holding all other variables constant. This is done by holding all variables constant for each observation in our training data set but then apply the unique values of `CRBI` for each observation. We then average the `Salary` across all the observations. 

```{r}
p1 <- partial(rf.fit, pred.var = "CRBI", 
              plot = TRUE, rug = TRUE, 
              plot.engine = "ggplot") + ggtitle("PDP (RF)")
p2 <- partial(gbm.fit, pred.var = "CRBI", 
              plot = TRUE, rug = TRUE, 
              plot.engine = "ggplot") + ggtitle("PDP (GBM)")
grid.arrange(p1, p2, nrow = 1)
```

# Classification

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. 

```{r}
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes
dat$diabetes <- factor(dat$diabetes, c("pos", "neg"))

set.seed(1)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 2/3,
                                list = FALSE)
```


## Bagging and random forests

```{r}
set.seed(1)
bagging <- randomForest(diabetes ~ . , 
                        dat[rowTrain,],
                        mtry = 8)

set.seed(1)
rf <- randomForest(diabetes ~ . , 
                   dat[rowTrain,],
                   mtry = 3)

set.seed(1)
rf2 <- ranger(diabetes ~ . , 
              dat[rowTrain,],
              mtry = 3, 
              probability = TRUE) 

rf.pred <- predict(rf, newdata = dat[-rowTrain,], type = "prob")[,1]
rf2.pred <- predict(rf2, data = dat[-rowTrain,], type = "response")$predictions[,1]
```

## Boosting

```{r}
# cannot pass categorical var to gbm => change to numeric first
dat2 <- dat
dat2$diabetes <- as.numeric(dat$diabetes == "pos")

set.seed(1)
bst <- gbm(diabetes ~ . , 
           dat2[rowTrain,],
           distribution = "adaboost",
           n.trees = 2000, 
           interaction.depth = 2,
           shrinkage = 0.005,
           cv.folds = 10,
           n.cores = 2)

gbm.perf(bst, method = "cv")
```

## Grid search using `caret`

### Random forests

```{r}
ctrl <- trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(1)
rf.fit <- train(diabetes ~ . , 
                dat, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```

### AdaBoost

```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)
set.seed(1)
gbmA.fit <- train(diabetes ~ . , 
                  dat, 
                  subset = rowTrain, 
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)

gbmA.pred <- predict(gbmA.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```


```{r}
resamp <- resamples(list(rf = rf.fit, 
                         gbmA = gbmA.fit))
summary(resamp)
```

## Global interpretation
### Variable importance

```{r}
set.seed(1)
rf2.final.per <- ranger(diabetes ~ . , 
                        dat[rowTrain,], 
                        mtry = rf.fit$bestTune[[1]], 
                        min.node.size = rf.fit$bestTune[[3]],
                        splitrule = "gini",
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

set.seed(1)
rf2.final.imp <- ranger(diabetes ~ . , dat[rowTrain,], 
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "gini",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))
```



```{r}
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### PDP 

```{r}
pdp.rf <- rf.fit %>% 
  partial(pred.var = "glucose", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = dat[rowTrain,]) +
  ggtitle("Random forest") 

pdp.gbm <- gbmA.fit %>% 
  partial(pred.var = "glucose", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = dat[rowTrain,]) +
  ggtitle("Boosting") 

grid.arrange(pdp.rf, pdp.gbm, nrow = 1)
```



```{r}
roc.rf <- roc(dat$diabetes[-rowTrain], rf.pred)
roc.gbmA <- roc(dat$diabetes[-rowTrain], gbmA.pred)

plot(roc.rf, col = 1)
plot(roc.gbmA, add = TRUE, col = 2)

auc <- c(roc.rf$auc[1], roc.gbmA$auc[1])

modelNames <- c("RF","Adaboost")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:2, lwd = 2)
```


