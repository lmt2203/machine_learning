---
title: "Ensemble Methods"
author: "Linh Tran"
output: 
   html_document:
     toc: true




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}

library(ISLR) # for data
library(mlbench) # for data


library(caret)
library(randomForest)  #RF
library(ranger)   #faster implementation of RF algorithm => good for high dimensional dataset

library(gbm)  #boosting (both general and binary)

library(pdp) #partial dependence plot

library(pROC)
```

# Regression

Predict a baseball playerâ€™s salary on the basis of various statistics associated with performance in the previous year. 

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)

set.seed(2021)
trRows <- createDataPartition(Hitters$Salary,
                              p = .75,
                              list = F)
```

## Bagging and Random forests

The function `randomForest()` implements Breiman's random forest algorithm (based on Breiman and Cutler's original Fortran code) for classification and regression. `ranger()` is a fast implementation of Breiman's random forests, particularly suited for high dimensional data.\

Bagging is a special case of RF if m = p

```{r}
set.seed(1)
bagging <- randomForest(Salary ~ . , 
                        Hitters,
                        subset = trRows,
                        mtry = 19)  #number of predictors 
#another argument: nodesize = min of obs within each terminal node default = 5, for classification default = 1 => growing very large tree by default

set.seed(1)
rf <- randomForest(Salary ~ . , 
                   Hitters,
                   subset = trRows,
                   mtry = 6)   #default mtry = p/3 

# fast implementation
set.seed(1)
rf2 <- ranger(Salary ~ . , 
              Hitters[trRows,],
              mtry = 6) 
#argument `min.node.size`

pred.rf <- predict(rf, newdata = Hitters[-trRows,]) #predict rf, output is vector
pred.rf2 <- predict(rf2, data = Hitters[-trRows,])$predictions #predict ranger, output is list => has to extract predictions using dollar sign then it will be vector
#caret: predict.train

RMSE(pred.rf, Hitters$Salary[-trRows])
RMSE(pred.rf2, Hitters$Salary[-trRows])
```


## Boosting

We first fit a gradient boosting model with Gaussian loss function.
Boosting has more tuning parameters and is more sensitive to model training => has to trian model more carefully.

```{r}
set.seed(1)

#`gbm` package

bst <- gbm(Salary ~ . , 
           Hitters[trRows,],
           distribution = "gaussian", #for regression
           n.trees = 5000, #B 
           interaction.depth = 3, #d = 1 by default
           shrinkage = 0.005, #lambda
           cv.folds = 10, #cv determines optimal n.trees
           n.cores = 2)
# bag.fraction = 0.5: 50% of training data is used to grow trees
```

We plot loss function as a result of number of trees added to the ensemble.

```{r}
gbm.perf(bst, method = "cv")
# x-axis = B (number of iteration), up to 5000
# 2 curves: 1 is training error (black), 1 is cv error (green). Select B that gives smallest cv error. = 1555 
```


## Grid search using `caret`

We use the fast implementation of random forest when tuning the model.

```{r}
ctrl <- trainControl(method = "cv") 

# Try more if possible
rf.grid <- expand.grid(mtry = 1:19, #number of selected variable at each split
                       splitrule = "variance",
                       min.node.size = 1:6) # min terminal node size, can try higher
                       
set.seed(1)
rf.fit <- train(Salary ~ . , 
                Hitters[trRows,], 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.fit$bestTune #extract using besttune from train object


```

We then tune the `gbm` model.Tuning more parameters: 

```{r}
# Try more 
gbm.grid <- expand.grid(n.trees = c(2000,3000,4000), #B
                        interaction.depth = 1:4, #d
                        shrinkage = c(0.001,0.003,0.005),  #lambda
                        n.minobsinnode = c(1,10))  #in caret used as tuning parameter
                      
                        
set.seed(1)
gbm.fit <- train(Salary ~ . , 
                 Hitters[trRows,], 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)

# n.minobsinnode = 1 works better
```

It takes a while to train the `gbm` even with a rough tuning grid. The `xgboost` package provides an efficient implementation of gradient boosting framework (approximately 10x faster than `gbm`). You can find much useful information here: https://github.com/dmlc/xgboost/tree/master/demo.

Compare the cross-validation performance. You can also compare with other models that we fitted before.

```{r}
resamp <- resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resamp)

#gbm gives smaller error
```


 
## Global interpretation
### Variable importance

We can extract the variable importance from the fitted models. In what follows, the first measure is computed from permuting OOB data. The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For regression, node impurity is measured by residual sum of squares.

```{r}
set.seed(1)
rf2.final.per <- ranger(Salary ~ . , 
                        Hitters[trRows,],
                        mtry = rf.fit$bestTune[[1]], #optimal tuning parameter
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]], #optimal tuning parameter
                        importance = "permutation",  #default setting importance = none
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19)) #generate 19 colors

set.seed(1)
rf2.final.imp <- ranger(Salary ~ . , 
                        Hitters[trRows,],
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))
```

Variable importance from boosting can be obtained using the `summary()` function.

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### Partial dependence plots 

After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. For this we can use partial dependence plots (PDPs).

PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. The PDP plot below displays the average change in predicted `Salary` as we vary `CRBI` while holding all other variables constant. This is done by holding all variables constant for each observation in our training data set but then apply the unique values of `CRBI` for each observation. We then average the `Salary` across all the observations. 

```{r}
p1 <- partial(rf.fit, pred.var = "CRBI", 
              plot = TRUE, rug = TRUE, 
              plot.engine = "ggplot") + ggtitle("PDP (RF)")
p2 <- partial(gbm.fit, pred.var = "CRBI", 
              plot = TRUE, rug = TRUE, 
              plot.engine = "ggplot") + ggtitle("PDP (GBM)")
grid.arrange(p1, p2, nrow = 1)
```

# Classification

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. 

```{r}
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes
dat$diabetes <- factor(dat$diabetes, c("pos", "neg"))

set.seed(1)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 2/3,
                                list = FALSE)
```


## Bagging and random forests

```{r}
set.seed(1)
bagging <- randomForest(diabetes ~ . , 
                        dat[rowTrain,],
                        mtry = 8)

set.seed(1)
rf <- randomForest(diabetes ~ . , 
                   dat[rowTrain,],
                   mtry = 3)

set.seed(1)
rf2 <- ranger(diabetes ~ . , 
              dat[rowTrain,],
              mtry = 3, 
              probability = TRUE) 

rf.pred <- predict(rf, newdata = dat[-rowTrain,], type = "prob")[,1]
rf2.pred <- predict(rf2, data = dat[-rowTrain,], type = "response")$predictions[,1]
```

## Boosting

```{r}
dat2 <- dat
dat2$diabetes <- as.numeric(dat$diabetes == "pos")

set.seed(1)
bst <- gbm(diabetes ~ . , 
           dat2[rowTrain,],
           distribution = "adaboost",
           n.trees = 2000, 
           interaction.depth = 2,
           shrinkage = 0.005,
           cv.folds = 10,
           n.cores = 2)

gbm.perf(bst, method = "cv")
```

## Grid search using `caret`

### Random forests

```{r}
ctrl <- trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(1)
rf.fit <- train(diabetes ~ . , 
                dat, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```

### AdaBoost

```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)
set.seed(1)
gbmA.fit <- train(diabetes ~ . , 
                  dat, 
                  subset = rowTrain, 
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)

gbmA.pred <- predict(gbmA.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```


```{r}
resamp <- resamples(list(rf = rf.fit, 
                         gbmA = gbmA.fit))
summary(resamp)
```

## Global interpretation
### Variable importance

```{r}
set.seed(1)
rf2.final.per <- ranger(diabetes ~ . , 
                        dat[rowTrain,], 
                        mtry = rf.fit$bestTune[[1]], 
                        min.node.size = rf.fit$bestTune[[3]],
                        splitrule = "gini",
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

set.seed(1)
rf2.final.imp <- ranger(diabetes ~ . , dat[rowTrain,], 
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "gini",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))
```



```{r}
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### PDP 

```{r}
pdp.rf <- rf.fit %>% 
  partial(pred.var = "glucose", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = dat[rowTrain,]) +
  ggtitle("Random forest") 

pdp.gbm <- gbmA.fit %>% 
  partial(pred.var = "glucose", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = dat[rowTrain,]) +
  ggtitle("Boosting") 

grid.arrange(pdp.rf, pdp.gbm, nrow = 1)
```



```{r}
roc.rf <- roc(dat$diabetes[-rowTrain], rf.pred)
roc.gbmA <- roc(dat$diabetes[-rowTrain], gbmA.pred)

plot(roc.rf, col = 1)
plot(roc.gbmA, add = TRUE, col = 2)

auc <- c(roc.rf$auc[1], roc.gbmA$auc[1])

modelNames <- c("RF","Adaboost")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:2, lwd = 2)
```


